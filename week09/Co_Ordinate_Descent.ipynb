{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mCewbxGuZDJF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_log_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DGngL8-lZJra"
   },
   "source": [
    "Tricks on optimization when you have large datasets\n",
    "1. FOR THE LOVE OF GOD, PLEASE DON'T USE CROSS-VALIDATION. The whole point of just using a validation set (vs cross-validation) is that it requires less computational time. Which is good when you have huge datasets as we do here.\n",
    "2. You should only do this if you really need to, but you can simply optimize on a sample of the training set rather than on the full dataset. Once you found the best parameters on the sample you simply need to retrain it on the full training set.\n",
    "\n",
    "Of course, ideally, you should always use cross-validation and you should also always use the full dataset rather than on sample. But it is not always possible.\n",
    "\n",
    "We will be using both of these tricks here.\n",
    "\n",
    "Note that the results here was done on Colab, and colab doesn't have a very good CPU. It was also all done within 1-2 hours, which is pretty quick. Moreover, if you can get XGBoost working with the GPU it should be even quicker. So in your assignments it should be expected that you run this code for even longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4gGwhfMQZGaL"
   },
   "outputs": [],
   "source": [
    "perth = pd.read_csv('perth_clean.csv')\n",
    "perth = pd.get_dummies(perth, columns=['suburb'])\n",
    "\n",
    "train_indices, test_indices = train_test_split(perth.index, test_size=0.2, random_state=0)\n",
    "\n",
    "perth_train = perth.loc[train_indices].copy()\n",
    "perth_test = perth.loc[test_indices].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wo5NP5LhdHwn",
    "outputId": "82fb84f8-0f18-497f-e5ea-00d4e42d11bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length 26924\n",
      "Tresting set length 6732\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set length\", len(perth_train))\n",
    "print(\"Tresting set length\", len(perth_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vd4zj69fZ4Ar",
    "outputId": "a8e3dfc9-5396-4629-bfab-c2f140d41b81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled set length 5384\n"
     ]
    }
   ],
   "source": [
    "indices = list(perth_train.index.copy())\n",
    "# np.random.seed(5)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "sampled_indices = indices[:int(len(indices) * 0.2)]\n",
    "sampled_perth = perth_train.loc[sampled_indices, :].copy()\n",
    "\n",
    "print(\"Sampled set length\", len(sampled_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_x_train = sampled_perth.drop('log10_price', axis=1)\n",
    "sampled_y_train = sampled_perth['log10_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7Mh6h-D_aXbY"
   },
   "outputs": [],
   "source": [
    "n = len(sampled_perth)\n",
    "sampled_train_indices, sampled_valid_indices = train_test_split(np.arange(n), test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mLFqjsUjdL1w",
    "outputId": "e2b46027-66ab-4089-96f0-6ce60519a2fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training length of sampled data 4307\n",
      "Validiation length of sampled data 1077\n"
     ]
    }
   ],
   "source": [
    "print(\"Training length of sampled data\", len(sampled_train_indices))\n",
    "print(\"Validiation length of sampled data\", len(sampled_valid_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLAorlcGeD4w"
   },
   "source": [
    "Just to give a brief recap of what we are doing here. \n",
    "\n",
    "1. Because of how big the dataset is, rather than using the entire training set (26924 length) we instead want to use a subset of that. I call this subset the sampled perth set (5384 length)\n",
    "2. On this sampled perth set you can then choose to use CV or just the validation approach. Because of how long the models take to train, we will be using the validation approach. \n",
    "\n",
    "    So the sampled perth set (5384 length) is then split into the sampled training set (4307 length) and sampled validation set (1077 length). We will then choose the best parameters based on this validation set (1077 length) \n",
    "3. Finally once we have choose the best hyper-parameter, we will need to retrain it on the full training set (26924 length). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCtv2ldia23T"
   },
   "source": [
    "# Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cYaPWLAjaOng",
    "outputId": "4a0654af-b199-4736-c078-6c74d7be67f6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 6, 'min_samples_leaf': 36}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid = {'min_samples_leaf': np.arange(1, 100, 5),\n",
    "                  'max_features': np.arange(1, len(perth_train.columns) - 1, 5)}\n",
    "\n",
    "base_parameters = {key: np.random.choice(val) for key, val in parameter_grid.items()}\n",
    "base_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kiQlLhk1a6ez",
    "outputId": "9c3c1421-7e5d-4d22-fd91-5713e5df0583"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 36, 'max_features': 6}\n",
      "Fitting 1 folds for each of 20 candidates, totalling 20 fits\n",
      "[CV 1/1] END ...............min_samples_leaf=1;, score=-0.011 total time=   1.3s\n",
      "[CV 1/1] END ...............min_samples_leaf=6;, score=-0.028 total time=   0.3s\n",
      "[CV 1/1] END ..............min_samples_leaf=11;, score=-0.040 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=16;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=21;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=26;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=31;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=36;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=41;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=46;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=51;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=56;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=61;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=66;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=71;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=76;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=81;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=86;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=91;, score=-0.045 total time=   0.2s\n",
      "[CV 1/1] END ..............min_samples_leaf=96;, score=-0.045 total time=   0.2s\n",
      "{'min_samples_leaf': 1, 'max_features': 6}\n",
      "Fitting 1 folds for each of 67 candidates, totalling 67 fits\n",
      "[CV 1/1] END ...................max_features=1;, score=-0.012 total time=   1.2s\n",
      "[CV 1/1] END ...................max_features=6;, score=-0.011 total time=   1.3s\n",
      "[CV 1/1] END ..................max_features=11;, score=-0.011 total time=   1.4s\n",
      "[CV 1/1] END ..................max_features=16;, score=-0.010 total time=   1.3s\n",
      "[CV 1/1] END ..................max_features=21;, score=-0.010 total time=   1.4s\n",
      "[CV 1/1] END ..................max_features=26;, score=-0.010 total time=   1.5s\n",
      "[CV 1/1] END ..................max_features=31;, score=-0.010 total time=   1.6s\n",
      "[CV 1/1] END ..................max_features=36;, score=-0.010 total time=   1.7s\n",
      "[CV 1/1] END ..................max_features=41;, score=-0.010 total time=   1.9s\n",
      "[CV 1/1] END ..................max_features=46;, score=-0.010 total time=   1.8s\n",
      "[CV 1/1] END ..................max_features=51;, score=-0.009 total time=   2.1s\n",
      "[CV 1/1] END ..................max_features=56;, score=-0.009 total time=   2.0s\n",
      "[CV 1/1] END ..................max_features=61;, score=-0.009 total time=   2.1s\n",
      "[CV 1/1] END ..................max_features=66;, score=-0.009 total time=   2.1s\n",
      "[CV 1/1] END ..................max_features=71;, score=-0.009 total time=   2.2s\n",
      "[CV 1/1] END ..................max_features=76;, score=-0.009 total time=   2.3s\n",
      "[CV 1/1] END ..................max_features=81;, score=-0.009 total time=   2.3s\n",
      "[CV 1/1] END ..................max_features=86;, score=-0.009 total time=   2.5s\n",
      "[CV 1/1] END ..................max_features=91;, score=-0.009 total time=   2.4s\n",
      "[CV 1/1] END ..................max_features=96;, score=-0.009 total time=   2.6s\n",
      "[CV 1/1] END .................max_features=101;, score=-0.009 total time=   2.6s\n",
      "[CV 1/1] END .................max_features=106;, score=-0.009 total time=   2.7s\n",
      "[CV 1/1] END .................max_features=111;, score=-0.009 total time=   2.7s\n",
      "[CV 1/1] END .................max_features=116;, score=-0.009 total time=   2.8s\n",
      "[CV 1/1] END .................max_features=121;, score=-0.009 total time=   2.9s\n",
      "[CV 1/1] END .................max_features=126;, score=-0.009 total time=   3.1s\n",
      "[CV 1/1] END .................max_features=131;, score=-0.009 total time=   3.2s\n",
      "[CV 1/1] END .................max_features=136;, score=-0.009 total time=   3.2s\n",
      "[CV 1/1] END .................max_features=141;, score=-0.009 total time=   3.3s\n",
      "[CV 1/1] END .................max_features=146;, score=-0.009 total time=   3.3s\n",
      "[CV 1/1] END .................max_features=151;, score=-0.009 total time=   3.4s\n",
      "[CV 1/1] END .................max_features=156;, score=-0.009 total time=   3.4s\n",
      "[CV 1/1] END .................max_features=161;, score=-0.009 total time=   3.5s\n",
      "[CV 1/1] END .................max_features=166;, score=-0.009 total time=   3.7s\n",
      "[CV 1/1] END .................max_features=171;, score=-0.009 total time=   3.7s\n",
      "[CV 1/1] END .................max_features=176;, score=-0.009 total time=   3.8s\n",
      "[CV 1/1] END .................max_features=181;, score=-0.009 total time=   3.9s\n",
      "[CV 1/1] END .................max_features=186;, score=-0.009 total time=   3.9s\n",
      "[CV 1/1] END .................max_features=191;, score=-0.009 total time=   4.0s\n",
      "[CV 1/1] END .................max_features=196;, score=-0.009 total time=   4.2s\n",
      "[CV 1/1] END .................max_features=201;, score=-0.009 total time=   4.2s\n",
      "[CV 1/1] END .................max_features=206;, score=-0.009 total time=   4.3s\n",
      "[CV 1/1] END .................max_features=211;, score=-0.009 total time=   4.4s\n",
      "[CV 1/1] END .................max_features=216;, score=-0.009 total time=   4.5s\n",
      "[CV 1/1] END .................max_features=221;, score=-0.009 total time=   4.6s\n",
      "[CV 1/1] END .................max_features=226;, score=-0.009 total time=   4.6s\n",
      "[CV 1/1] END .................max_features=231;, score=-0.009 total time=   4.6s\n",
      "[CV 1/1] END .................max_features=236;, score=-0.009 total time=   4.7s\n",
      "[CV 1/1] END .................max_features=241;, score=-0.010 total time=   4.8s\n",
      "[CV 1/1] END .................max_features=246;, score=-0.009 total time=   4.9s\n",
      "[CV 1/1] END .................max_features=251;, score=-0.010 total time=   4.9s\n",
      "[CV 1/1] END .................max_features=256;, score=-0.010 total time=   5.0s\n",
      "[CV 1/1] END .................max_features=261;, score=-0.009 total time=   5.2s\n",
      "[CV 1/1] END .................max_features=266;, score=-0.010 total time=   5.3s\n",
      "[CV 1/1] END .................max_features=271;, score=-0.010 total time=   5.3s\n",
      "[CV 1/1] END .................max_features=276;, score=-0.010 total time=   5.4s\n",
      "[CV 1/1] END .................max_features=281;, score=-0.010 total time=   5.5s\n",
      "[CV 1/1] END .................max_features=286;, score=-0.010 total time=   5.4s\n",
      "[CV 1/1] END .................max_features=291;, score=-0.010 total time=   5.6s\n",
      "[CV 1/1] END .................max_features=296;, score=-0.010 total time=   5.7s\n",
      "[CV 1/1] END .................max_features=301;, score=-0.010 total time=   5.7s\n",
      "[CV 1/1] END .................max_features=306;, score=-0.010 total time=   5.7s\n",
      "[CV 1/1] END .................max_features=311;, score=-0.010 total time=   5.9s\n",
      "[CV 1/1] END .................max_features=316;, score=-0.010 total time=   5.9s\n",
      "[CV 1/1] END .................max_features=321;, score=-0.010 total time=   6.0s\n",
      "[CV 1/1] END .................max_features=326;, score=-0.010 total time=   6.1s\n",
      "[CV 1/1] END .................max_features=331;, score=-0.010 total time=   6.2s\n",
      "{'min_samples_leaf': 1, 'max_features': 106}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)\n",
    "for parameter, grid in parameter_grid.items():\n",
    "    base_model = RandomForestRegressor(criterion='squared_error', random_state=0, **base_parameters)\n",
    "    \n",
    "    parameter_sub_grid = {parameter: grid}\n",
    "    search = GridSearchCV(base_model, parameter_sub_grid, \n",
    "                          cv=[(sampled_train_indices, sampled_valid_indices)], \n",
    "                          scoring=\"neg_mean_squared_error\", verbose=3, refit=False)\n",
    "    \n",
    "    search.fit(sampled_x_train, sampled_y_train)\n",
    "    \n",
    "    base_parameters.update(search.best_params_)\n",
    "    print(base_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1AUX3aEcnYk"
   },
   "source": [
    "Again but smarter. So we can simply run this again with the same parameter grid\n",
    "\n",
    "    parameter_grid = {'min_samples_leaf': np.arange(1, 100, 5),\n",
    "                      'max_features': np.arange(1, len(perth_train.columns) - 1, 5)}\n",
    "\n",
    "But when we look at the output a bit more carefully, we can see that for `min_samples_leaf` the error increases drastically after 6. So there is no reason why we need to look any thing more than 6. As such we can focus the search on this interval\n",
    "\n",
    "    parameter_grid = {'min_samples_leaf': np.arange(1, 6),\n",
    "                      'max_features': np.arange(1, len(perth_train.columns) - 1, 5)}\n",
    "\n",
    "\n",
    "For `max_features` we have to be a bit careful. Recall we are doing validation, not cross-validation, as such the validation error tends to be noisy. We can see that for the most part the scores are all $\\pm 0.01$ from the best score $0.09$. So we will just leave it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1nB6RIOcxp0",
    "outputId": "847fb992-bbe1-470f-8f37-7bd475f19ab4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 1, 'max_features': 106}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zeyjGBWc5tT"
   },
   "outputs": [],
   "source": [
    "parameter_grid = {'min_samples_leaf': np.arange(1, 6),\n",
    "                  'max_features': np.arange(1, len(perth_train.columns) - 1, 5)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecWV7t1Qa-vc",
    "outputId": "eb8541a9-4de7-40e6-f4f6-cc41cec78914"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_samples_leaf': 1, 'max_features': 106}\n",
      "Fitting 1 folds for each of 5 candidates, totalling 5 fits\n",
      "[CV 1/1] END ...............min_samples_leaf=1;, score=-0.009 total time=   2.7s\n",
      "[CV 1/1] END ...............min_samples_leaf=2;, score=-0.009 total time=   2.2s\n",
      "[CV 1/1] END ...............min_samples_leaf=3;, score=-0.010 total time=   1.9s\n",
      "[CV 1/1] END ...............min_samples_leaf=4;, score=-0.010 total time=   1.8s\n",
      "[CV 1/1] END ...............min_samples_leaf=5;, score=-0.010 total time=   1.7s\n",
      "{'min_samples_leaf': 1, 'max_features': 106}\n",
      "Fitting 1 folds for each of 67 candidates, totalling 67 fits\n",
      "[CV 1/1] END ...................max_features=1;, score=-0.012 total time=   1.3s\n",
      "[CV 1/1] END ...................max_features=6;, score=-0.011 total time=   1.2s\n",
      "[CV 1/1] END ..................max_features=11;, score=-0.011 total time=   1.2s\n",
      "[CV 1/1] END ..................max_features=16;, score=-0.010 total time=   1.3s\n",
      "[CV 1/1] END ..................max_features=21;, score=-0.010 total time=   1.4s\n",
      "[CV 1/1] END ..................max_features=26;, score=-0.010 total time=   1.5s\n",
      "[CV 1/1] END ..................max_features=31;, score=-0.010 total time=   1.6s\n",
      "[CV 1/1] END ..................max_features=36;, score=-0.010 total time=   1.6s\n",
      "[CV 1/1] END ..................max_features=41;, score=-0.010 total time=   1.8s\n",
      "[CV 1/1] END ..................max_features=46;, score=-0.010 total time=   1.9s\n",
      "[CV 1/1] END ..................max_features=51;, score=-0.009 total time=   2.0s\n",
      "[CV 1/1] END ..................max_features=56;, score=-0.009 total time=   2.0s\n",
      "[CV 1/1] END ..................max_features=61;, score=-0.009 total time=   2.0s\n",
      "[CV 1/1] END ..................max_features=66;, score=-0.009 total time=   2.2s\n",
      "[CV 1/1] END ..................max_features=71;, score=-0.009 total time=   2.2s\n",
      "[CV 1/1] END ..................max_features=76;, score=-0.009 total time=   2.3s\n",
      "[CV 1/1] END ..................max_features=81;, score=-0.009 total time=   2.3s\n",
      "[CV 1/1] END ..................max_features=86;, score=-0.009 total time=   2.5s\n",
      "[CV 1/1] END ..................max_features=91;, score=-0.009 total time=   2.4s\n",
      "[CV 1/1] END ..................max_features=96;, score=-0.009 total time=   2.5s\n",
      "[CV 1/1] END .................max_features=101;, score=-0.009 total time=   2.6s\n",
      "[CV 1/1] END .................max_features=106;, score=-0.009 total time=   2.6s\n",
      "[CV 1/1] END .................max_features=111;, score=-0.009 total time=   2.8s\n",
      "[CV 1/1] END .................max_features=116;, score=-0.009 total time=   2.8s\n",
      "[CV 1/1] END .................max_features=121;, score=-0.009 total time=   2.8s\n",
      "[CV 1/1] END .................max_features=126;, score=-0.009 total time=   2.9s\n",
      "[CV 1/1] END .................max_features=131;, score=-0.009 total time=   3.0s\n",
      "[CV 1/1] END .................max_features=136;, score=-0.009 total time=   3.1s\n",
      "[CV 1/1] END .................max_features=141;, score=-0.009 total time=   3.2s\n",
      "[CV 1/1] END .................max_features=146;, score=-0.009 total time=   3.2s\n",
      "[CV 1/1] END .................max_features=151;, score=-0.009 total time=   3.4s\n",
      "[CV 1/1] END .................max_features=156;, score=-0.009 total time=   3.4s\n",
      "[CV 1/1] END .................max_features=161;, score=-0.009 total time=   3.5s\n",
      "[CV 1/1] END .................max_features=166;, score=-0.009 total time=   3.6s\n",
      "[CV 1/1] END .................max_features=171;, score=-0.009 total time=   3.6s\n",
      "[CV 1/1] END .................max_features=176;, score=-0.009 total time=   3.6s\n",
      "[CV 1/1] END .................max_features=181;, score=-0.009 total time=   3.8s\n",
      "[CV 1/1] END .................max_features=186;, score=-0.009 total time=   3.9s\n",
      "[CV 1/1] END .................max_features=191;, score=-0.009 total time=   3.9s\n",
      "[CV 1/1] END .................max_features=196;, score=-0.009 total time=   4.1s\n",
      "[CV 1/1] END .................max_features=201;, score=-0.009 total time=   4.1s\n",
      "[CV 1/1] END .................max_features=206;, score=-0.009 total time=   4.2s\n",
      "[CV 1/1] END .................max_features=211;, score=-0.009 total time=   4.2s\n",
      "[CV 1/1] END .................max_features=216;, score=-0.009 total time=   4.4s\n",
      "[CV 1/1] END .................max_features=221;, score=-0.009 total time=   4.4s\n",
      "[CV 1/1] END .................max_features=226;, score=-0.009 total time=   4.5s\n",
      "[CV 1/1] END .................max_features=231;, score=-0.009 total time=   4.6s\n",
      "[CV 1/1] END .................max_features=236;, score=-0.009 total time=   4.6s\n",
      "[CV 1/1] END .................max_features=241;, score=-0.010 total time=   4.7s\n",
      "[CV 1/1] END .................max_features=246;, score=-0.009 total time=   4.7s\n",
      "[CV 1/1] END .................max_features=251;, score=-0.010 total time=   4.9s\n",
      "[CV 1/1] END .................max_features=256;, score=-0.010 total time=   4.9s\n",
      "[CV 1/1] END .................max_features=261;, score=-0.009 total time=   5.1s\n",
      "[CV 1/1] END .................max_features=266;, score=-0.010 total time=   5.1s\n",
      "[CV 1/1] END .................max_features=271;, score=-0.010 total time=   5.3s\n",
      "[CV 1/1] END .................max_features=276;, score=-0.010 total time=   5.3s\n",
      "[CV 1/1] END .................max_features=281;, score=-0.010 total time=   5.4s\n",
      "[CV 1/1] END .................max_features=286;, score=-0.010 total time=   5.4s\n",
      "[CV 1/1] END .................max_features=291;, score=-0.010 total time=   5.6s\n",
      "[CV 1/1] END .................max_features=296;, score=-0.010 total time=   5.5s\n",
      "[CV 1/1] END .................max_features=301;, score=-0.010 total time=   5.6s\n",
      "[CV 1/1] END .................max_features=306;, score=-0.010 total time=   5.6s\n",
      "[CV 1/1] END .................max_features=311;, score=-0.010 total time=   5.8s\n",
      "[CV 1/1] END .................max_features=316;, score=-0.010 total time=   5.9s\n",
      "[CV 1/1] END .................max_features=321;, score=-0.010 total time=   5.9s\n",
      "[CV 1/1] END .................max_features=326;, score=-0.010 total time=   6.0s\n",
      "[CV 1/1] END .................max_features=331;, score=-0.010 total time=   6.1s\n",
      "{'min_samples_leaf': 1, 'max_features': 106}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)\n",
    "for parameter, grid in parameter_grid.items():\n",
    "    base_model = RandomForestRegressor(criterion='squared_error', random_state=0, **base_parameters)\n",
    "    \n",
    "    parameter_sub_grid = {parameter: grid}\n",
    "    search = GridSearchCV(base_model, parameter_sub_grid, \n",
    "                          cv=[(sampled_train_indices, sampled_valid_indices)], \n",
    "                          scoring=\"neg_mean_squared_error\", verbose=3, refit=False)\n",
    "    \n",
    "    search.fit(sampled_x_train, sampled_y_train)\n",
    "    \n",
    "    base_parameters.update(search.best_params_)\n",
    "    print(base_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TqmkLri_edt3"
   },
   "source": [
    "### Importantly, once you found the best parameters you need to retrain the final model on the FULL dataset.\n",
    "\n",
    "As an aside, look how long it takes to train on the full dataset. Now imagine doing this over and over again to find the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OaEtCeBGenC6",
    "outputId": "aa12370d-4336-469f-925e-81d6ab4b1b95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_features': 106, 'min_samples_leaf': 1}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56uligD4bAlD",
    "outputId": "5ee5e7db-dd56-497f-9999-0e89987413ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00781962457241284\n",
      "CPU times: user 26.2 s, sys: 55 ms, total: 26.2 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_model = RandomForestRegressor(criterion='squared_error', random_state=0, **base_parameters)\n",
    "final_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = final_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UH7Dnm9e9pi"
   },
   "source": [
    "Just to prove the point. We are sampling the dataset when we do the optimization. So it is very likely that the final parameters are not the best when considering the full dataset. But let us see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7052O6HKe7RR",
    "outputId": "4f405d8b-7a39-4556-bb55-3f7553a752a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008252400627787577\n"
     ]
    }
   ],
   "source": [
    "# Change min_samples_leaf\n",
    "\n",
    "new_parameters = {'min_samples_leaf': 6, 'max_features': 106}\n",
    "\n",
    "new_model = RandomForestRegressor(criterion='squared_error', random_state=0, **new_parameters)\n",
    "new_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = new_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dukCDFNOfdMN",
    "outputId": "0d2ad429-caef-407a-b677-cddfa8d1b374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007924114544086851\n"
     ]
    }
   ],
   "source": [
    "# Change max_features\n",
    "\n",
    "new_parameters = {'min_samples_leaf': 1, 'max_features': 241}\n",
    "\n",
    "new_model = RandomForestRegressor(criterion='squared_error', random_state=0, **new_parameters)\n",
    "new_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = new_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BEdvVOVf_ha"
   },
   "source": [
    "# XGBoost\n",
    "\n",
    "Note that XGBoost can use a GPU and Colab does give you a free GPU. But I can't figure out how to get that working. So I'll let that for you too look at, but it should make things MUCH MUCH quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gLYHqpmgftV2",
    "outputId": "5b05387e-63c2-4a39-8e4c-89e51f174235"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.06000000000000001,\n",
       " 'max_depth': 81,\n",
       " 'n_estimators': 60,\n",
       " 'subsample': 0.5}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid = {'n_estimators': np.arange(50, 100 + 1, 10),\n",
    "                  'learning_rate': np.linspace(0.01, 0.10, 10),\n",
    "                  'max_depth': np.arange(1, 100 + 1, 10),\n",
    "                  'subsample': np.linspace(0.1, 1.0, 10)}\n",
    "\n",
    "# np.random.seed(5)\n",
    "base_parameters = {key: np.random.choice(val) for key, val in parameter_grid.items()}\n",
    "base_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kxo1bDifgegA",
    "outputId": "f2c9f77b-934f-4239-d66f-2105f2839c42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 60, 'learning_rate': 0.06000000000000001, 'max_depth': 81, 'subsample': 0.5}\n",
      "Fitting 1 folds for each of 6 candidates, totalling 6 fits\n",
      "[CV 1/1] END ..................n_estimators=50;, score=-0.070 total time=   3.8s\n",
      "[CV 1/1] END ..................n_estimators=60;, score=-0.028 total time=   5.9s\n",
      "[CV 1/1] END ..................n_estimators=70;, score=-0.015 total time=   8.2s\n",
      "[CV 1/1] END ..................n_estimators=80;, score=-0.011 total time=  11.1s\n",
      "[CV 1/1] END ..................n_estimators=90;, score=-0.009 total time=  13.8s\n",
      "[CV 1/1] END .................n_estimators=100;, score=-0.009 total time=  16.7s\n",
      "{'n_estimators': 100, 'learning_rate': 0.06000000000000001, 'max_depth': 81, 'subsample': 0.5}\n",
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[CV 1/1] END ...............learning_rate=0.01;, score=-3.717 total time=   2.9s\n",
      "[CV 1/1] END learning_rate=0.020000000000000004;, score=-0.507 total time=   4.4s\n",
      "[CV 1/1] END learning_rate=0.030000000000000006;, score=-0.077 total time=   7.1s\n",
      "[CV 1/1] END learning_rate=0.04000000000000001;, score=-0.019 total time=  10.8s\n",
      "[CV 1/1] END learning_rate=0.05000000000000001;, score=-0.010 total time=  14.5s\n",
      "[CV 1/1] END learning_rate=0.06000000000000001;, score=-0.009 total time=  16.7s\n",
      "[CV 1/1] END ...............learning_rate=0.07;, score=-0.009 total time=  19.4s\n",
      "[CV 1/1] END ...............learning_rate=0.08;, score=-0.009 total time=  21.8s\n",
      "[CV 1/1] END learning_rate=0.09000000000000001;, score=-0.009 total time=  24.0s\n",
      "[CV 1/1] END ................learning_rate=0.1;, score=-0.009 total time=  26.3s\n",
      "{'n_estimators': 100, 'learning_rate': 0.09000000000000001, 'max_depth': 81, 'subsample': 0.5}\n",
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[CV 1/1] END ......................max_depth=1;, score=-0.016 total time=   1.6s\n",
      "[CV 1/1] END .....................max_depth=11;, score=-0.009 total time=   9.8s\n",
      "[CV 1/1] END .....................max_depth=21;, score=-0.009 total time=  16.1s\n",
      "[CV 1/1] END .....................max_depth=31;, score=-0.009 total time=  20.3s\n",
      "[CV 1/1] END .....................max_depth=41;, score=-0.009 total time=  22.7s\n",
      "[CV 1/1] END .....................max_depth=51;, score=-0.009 total time=  23.7s\n",
      "[CV 1/1] END .....................max_depth=61;, score=-0.009 total time=  24.0s\n",
      "[CV 1/1] END .....................max_depth=71;, score=-0.009 total time=  24.0s\n",
      "[CV 1/1] END .....................max_depth=81;, score=-0.009 total time=  24.0s\n",
      "[CV 1/1] END .....................max_depth=91;, score=-0.009 total time=  24.1s\n",
      "{'n_estimators': 100, 'learning_rate': 0.09000000000000001, 'max_depth': 11, 'subsample': 0.5}\n",
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[CV 1/1] END ....................subsample=0.1;, score=-0.010 total time=   4.6s\n",
      "[CV 1/1] END ....................subsample=0.2;, score=-0.009 total time=   6.2s\n",
      "[CV 1/1] END ....subsample=0.30000000000000004;, score=-0.009 total time=   7.7s\n",
      "[CV 1/1] END ....................subsample=0.4;, score=-0.009 total time=   9.1s\n",
      "[CV 1/1] END ....................subsample=0.5;, score=-0.009 total time=   9.8s\n",
      "[CV 1/1] END ....................subsample=0.6;, score=-0.009 total time=   9.7s\n",
      "[CV 1/1] END .....subsample=0.7000000000000001;, score=-0.009 total time=   9.3s\n",
      "[CV 1/1] END ....................subsample=0.8;, score=-0.008 total time=   8.6s\n",
      "[CV 1/1] END ....................subsample=0.9;, score=-0.008 total time=   8.0s\n",
      "[CV 1/1] END ....................subsample=1.0;, score=-0.009 total time=   7.2s\n",
      "{'n_estimators': 100, 'learning_rate': 0.09000000000000001, 'max_depth': 11, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)\n",
    "for parameter, grid in parameter_grid.items():\n",
    "    base_model = XGBRegressor(objective='reg:squarederror', random_state=0, **base_parameters)\n",
    "    \n",
    "    parameter_sub_grid = {parameter: grid}\n",
    "    search = GridSearchCV(base_model, parameter_sub_grid, \n",
    "                          cv=[(sampled_train_indices, sampled_valid_indices)], \n",
    "                          scoring=\"neg_mean_squared_error\", verbose=3, refit=False)\n",
    "    \n",
    "    search.fit(sampled_x_train, sampled_y_train)\n",
    "    \n",
    "    base_parameters.update(search.best_params_)\n",
    "    print(base_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8uL5cItlhR39"
   },
   "source": [
    "Here we now have something interesting. We can see that for `n_estimators` that the score keeps decreasing. This is hinting that we haven't reached the bottom of the validation curve yet, we are still in the underfitting/overfitting region (but here we are actually underfitting). This is also reinforced by the fact that the optimal `n_estimators` is actually the last value we tried. So this is telling us that we should try larger values of `n_estimators`. \n",
    "\n",
    "Note that a similar story can be said for the other parameters. But it is complicated by the fact that the optimal `learning_rate` and `subsample` is not actually the last value. This implies that we should focus the search near these points but also try larger values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ywnSm6-gli1",
    "outputId": "18774b65-57eb-4faa-ac6f-40acc5936549"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.09000000000000001, 'max_depth': 11, 'subsample': 0.8}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZbSpfUYCiT-t",
    "outputId": "0bb43f15-bf04-4df1-b8f4-2f31c9feba34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': array([0.07, 0.08, 0.09, 0.1 , 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17,\n",
       "        0.18, 0.19]),\n",
       " 'max_depth': array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91]),\n",
       " 'n_estimators': array([100, 110, 120, 130, 140, 150, 160, 170, 180, 190, 200]),\n",
       " 'subsample': array([0.5, 0.6, 0.7, 0.8, 0.9])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameter_grid = {'n_estimators': np.arange(100, 200 + 1, 10),\n",
    "                  'learning_rate': np.arange(0.07, 0.2, 0.01),\n",
    "                  'max_depth': np.arange(1, 100 + 1, 10),\n",
    "                  'subsample': np.arange(0.5, 1, 0.1)}\n",
    "\n",
    "parameter_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r03WaFxnjZLn",
    "outputId": "72e21e9d-35e7-4e5c-ba77-8d711eaecf44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 100, 'learning_rate': 0.09000000000000001, 'max_depth': 11, 'subsample': 0.8}\n",
      "Fitting 1 folds for each of 11 candidates, totalling 11 fits\n",
      "[CV 1/1] END .................n_estimators=100;, score=-0.010 total time=  11.3s\n",
      "[CV 1/1] END .................n_estimators=110;, score=-0.010 total time=  12.4s\n",
      "[CV 1/1] END .................n_estimators=120;, score=-0.010 total time=  13.6s\n",
      "[CV 1/1] END .................n_estimators=130;, score=-0.010 total time=  14.7s\n",
      "[CV 1/1] END .................n_estimators=140;, score=-0.010 total time=  15.9s\n",
      "[CV 1/1] END .................n_estimators=150;, score=-0.010 total time=  17.1s\n",
      "[CV 1/1] END .................n_estimators=160;, score=-0.010 total time=  18.3s\n",
      "[CV 1/1] END .................n_estimators=170;, score=-0.010 total time=  19.5s\n",
      "[CV 1/1] END .................n_estimators=180;, score=-0.010 total time=  20.7s\n",
      "[CV 1/1] END .................n_estimators=190;, score=-0.010 total time=  22.0s\n",
      "[CV 1/1] END .................n_estimators=200;, score=-0.010 total time=  23.3s\n",
      "{'n_estimators': 200, 'learning_rate': 0.09000000000000001, 'max_depth': 11, 'subsample': 0.8}\n",
      "Fitting 1 folds for each of 13 candidates, totalling 13 fits\n",
      "[CV 1/1] END ...............learning_rate=0.07;, score=-0.010 total time=  22.9s\n",
      "[CV 1/1] END ...............learning_rate=0.08;, score=-0.010 total time=  23.0s\n",
      "[CV 1/1] END ...............learning_rate=0.09;, score=-0.010 total time=  23.1s\n",
      "[CV 1/1] END learning_rate=0.09999999999999999;, score=-0.009 total time=  23.1s\n",
      "[CV 1/1] END learning_rate=0.10999999999999999;, score=-0.010 total time=  23.3s\n",
      "[CV 1/1] END learning_rate=0.11999999999999998;, score=-0.010 total time=  23.3s\n",
      "[CV 1/1] END learning_rate=0.12999999999999998;, score=-0.009 total time=  23.4s\n",
      "[CV 1/1] END learning_rate=0.13999999999999996;, score=-0.010 total time=  23.4s\n",
      "[CV 1/1] END learning_rate=0.14999999999999997;, score=-0.010 total time=  23.4s\n",
      "[CV 1/1] END learning_rate=0.15999999999999998;, score=-0.010 total time=  23.4s\n",
      "[CV 1/1] END learning_rate=0.16999999999999996;, score=-0.010 total time=  23.5s\n",
      "[CV 1/1] END learning_rate=0.17999999999999994;, score=-0.010 total time=  23.5s\n",
      "[CV 1/1] END learning_rate=0.18999999999999995;, score=-0.010 total time=  23.5s\n",
      "{'n_estimators': 200, 'learning_rate': 0.12999999999999998, 'max_depth': 11, 'subsample': 0.8}\n",
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n",
      "[CV 1/1] END ......................max_depth=1;, score=-0.012 total time=   3.4s\n",
      "[CV 1/1] END .....................max_depth=11;, score=-0.009 total time=  23.3s\n",
      "[CV 1/1] END .....................max_depth=21;, score=-0.010 total time=  35.5s\n",
      "[CV 1/1] END .....................max_depth=31;, score=-0.010 total time=  39.8s\n",
      "[CV 1/1] END .....................max_depth=41;, score=-0.010 total time=  45.3s\n",
      "[CV 1/1] END .....................max_depth=51;, score=-0.010 total time=  49.0s\n",
      "[CV 1/1] END .....................max_depth=61;, score=-0.010 total time=  50.0s\n",
      "[CV 1/1] END .....................max_depth=71;, score=-0.010 total time=  50.8s\n",
      "[CV 1/1] END .....................max_depth=81;, score=-0.010 total time=  50.7s\n",
      "[CV 1/1] END .....................max_depth=91;, score=-0.010 total time=  50.9s\n",
      "{'n_estimators': 200, 'learning_rate': 0.12999999999999998, 'max_depth': 11, 'subsample': 0.8}\n",
      "Fitting 1 folds for each of 5 candidates, totalling 5 fits\n",
      "[CV 1/1] END ....................subsample=0.5;, score=-0.010 total time=  27.4s\n",
      "[CV 1/1] END ....................subsample=0.6;, score=-0.010 total time=  27.0s\n",
      "[CV 1/1] END ....................subsample=0.7;, score=-0.010 total time=  25.5s\n",
      "[CV 1/1] END .....subsample=0.7999999999999999;, score=-0.009 total time=  23.4s\n",
      "[CV 1/1] END .....subsample=0.8999999999999999;, score=-0.010 total time=  21.2s\n",
      "{'n_estimators': 200, 'learning_rate': 0.12999999999999998, 'max_depth': 11, 'subsample': 0.7999999999999999}\n"
     ]
    }
   ],
   "source": [
    "print(base_parameters)\n",
    "for parameter, grid in parameter_grid.items():\n",
    "    base_model = XGBRegressor(objective='reg:squarederror', random_state=0, **base_parameters)\n",
    "    \n",
    "    parameter_sub_grid = {parameter: grid}\n",
    "    search = GridSearchCV(base_model, parameter_sub_grid, \n",
    "                          cv=[(sampled_train_indices, sampled_valid_indices)], \n",
    "                          scoring=\"neg_mean_squared_error\", verbose=3, refit=False)\n",
    "    \n",
    "    search.fit(sampled_x_train, sampled_y_train)\n",
    "    \n",
    "    base_parameters.update(search.best_params_)\n",
    "    print(base_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1mb2FpjW_XA"
   },
   "source": [
    "I would suggest that you re-do the co-ordinate descent again. But we will just leave it as it is.\n",
    "\n",
    "Now we train on the best parameters. Notice how long this takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HERVmSqtjg_4",
    "outputId": "babd6f65-bc22-47f0-93e7-7d7f112679b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.12999999999999998,\n",
       " 'max_depth': 11,\n",
       " 'n_estimators': 200,\n",
       " 'subsample': 0.7999999999999999}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d67Lw4BlW20N",
    "outputId": "472a1dde-e9d1-4897-93e2-3836e1d2534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0075060996446797215\n",
      "CPU times: user 2min 25s, sys: 206 ms, total: 2min 25s\n",
      "Wall time: 2min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "final_model = XGBRegressor(objective='reg:squarederror', random_state=0, **base_parameters)\n",
    "final_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = final_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HonyBrJW9Xk"
   },
   "source": [
    "Again just to prove the point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUvS1xxOW8p_",
    "outputId": "5583fe9b-bfa3-44ce-f039-2e29c93d0cac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.007741369468912832\n",
      "CPU times: user 2min 55s, sys: 220 ms, total: 2min 55s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Change subsample\n",
    "new_parameters = {'n_estimators': 200, 'learning_rate': 0.12999999999999998, \n",
    "                  'max_depth': 11, 'subsample': 0.5}\n",
    "new_model = XGBRegressor(objective='reg:squarederror', random_state=0, **new_parameters)\n",
    "new_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = new_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXNoM1bPXQmm",
    "outputId": "3561aa5f-1962-4dee-8157-7f3b9c7af634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0078446913934133\n",
      "CPU times: user 4min 39s, sys: 275 ms, total: 4min 39s\n",
      "Wall time: 4min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Change Max Depth\n",
    "new_parameters = {'n_estimators': 200, 'learning_rate': 0.12999999999999998, \n",
    "                  'max_depth': 21, 'subsample': 0.7999999999999999}\n",
    "new_model = XGBRegressor(objective='reg:squarederror', random_state=0, **new_parameters)\n",
    "new_model.fit(perth_train.drop('log10_price', axis=1), perth_train['log10_price'])\n",
    "\n",
    "y = perth_test['log10_price']\n",
    "y_pred = new_model.predict(perth_test.drop('log10_price', axis=1))\n",
    "\n",
    "print(np.mean((y - y_pred) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pj7IoTZUXWQ4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Co-Ordinate Descent.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
