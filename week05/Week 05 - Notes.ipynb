{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://eng.uber.com/deepeta-how-uber-predicts-arrival-times/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "### Confusion Matrix and Im-balanced Datasets\n",
    "\n",
    "Suppose I told you my model had a 98% accuracy rate. The question is, is the model good or bad?\n",
    "\n",
    "The real answer is that it depends. If I told you that in the dataset, 98% had the label 0 and 2% had the label 1 this result is not impressive anymore (highly imbalanced data). Because your model can simply be $\\hat{y} = 0$ and you get a 98% accuracy rate. Now lets look at the confusion matrix\n",
    "\n",
    "|            | $Y_{pred} = 0$ | $Y_{pred} = 1$ |\n",
    "|------------|--------------|--------------|\n",
    "|$Y_{true} = 0$|        True Negatives     |     False Positives      | \n",
    "|$Y_{true} = 1$|  False Negatives         |      True Positives       |\n",
    "\n",
    "$$\\textrm{Accuracy} = \\frac{\\textrm{True Negatives} + \\textrm{True Positives}}{\\textrm{Number of Samples}}$$\n",
    "\n",
    "$$ \\textrm{Precision} = \\frac{\\textrm{True Positives}}{\\textrm{True Positives + False Positives}}$$\n",
    "\n",
    "\n",
    "### Ideal Model (Accuracy 100%)\n",
    "\n",
    "|            | $Y_{pred} = 0$ | $Y_{pred} = 1$ |\n",
    "|------------|--------------|--------------|\n",
    "|$Y_{true} = 0$|        98     |     0      | \n",
    "|$Y_{true} = 1$|  0        |      2       |\n",
    "\n",
    "\n",
    "$$ \\textrm{Precision} = \\frac{2}{2}=1$$\n",
    "<br>\n",
    "<br>\n",
    "\n",
    " \n",
    "### Model: `y = 0` (Accuracy 98%)\n",
    "\n",
    "|            | $Y_{pred} = 0$ | $Y_{pred} = 1$ |\n",
    "|------------|--------------|--------------|\n",
    "|$Y_{true} = 0$|        98     |     0      | \n",
    "|$Y_{true} = 1$|  2         |      0       |\n",
    "\n",
    "\n",
    "$$ \\textrm{Precision} = \\frac{0}{0 + 0}=... \\textrm{math}...=0$$\n",
    "<br>\n",
    "<br>\n",
    "### Model: Flipping a Coin (Accuracy 50%)\n",
    "|            | $Y_{pred} = 0$ | $Y_{pred} = 1$ |\n",
    "|------------|--------------|--------------|\n",
    "|$Y_{true} = 0$|        49     |     49      | \n",
    "|$Y_{true} = 1$|  1         |      1       |\n",
    "\n",
    "$$ \\textrm{Precision} = \\frac{1}{1 + 49}=0.02$$\n",
    "\n",
    "\n",
    "Now, if I ask you that this data was on cancer cells\n",
    "- label 0: No Cancer\n",
    "- label 1: Has Cancer\n",
    "\n",
    "It would seem that flipping a coin (50% accuracy) is much better than 99% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = pd.read_csv('german_credit.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.7\n",
       "1    0.3\n",
       "Name: default, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"default\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.679\n",
       "1    0.321\n",
       "Name: default, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['default'].value_counts(normalize=True).round(3)\n",
    "\n",
    "index_train, index_valid = train_test_split(data.index, train_size=0.7, random_state=189)\n",
    "train = data.loc[index_train,:].copy()\n",
    "valid = data.loc[index_valid,:].copy()\n",
    "\n",
    "train['default'].value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.75\n",
       "1    0.25\n",
       "Name: default, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid['default'].value_counts(normalize=True).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Business understanding\n",
    "\n",
    "The data documentation specifies the following loss matrix: \n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/ Predicted</th>\n",
    "    <th>Repayment</th>\n",
    "     <th>Default</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Repayment</th>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Default</th>\n",
    "    <td>5</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "That is, if we predict a default but the client is creditworthy, the loss is 1.  If we predict that the client will repay the loan but there is a default, the loss is 5. The loss for a correct classification is 0. Using general classificatioon terminology, we say that the loss from a false positive is 1, the loss from a false negative is 5, and the loss from both true positives and true negatives is zero. \n",
    "\n",
    "Using the formula from the lecture, the decision threshold is: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we essentially say that \n",
    "1. If the true value was default and we predict repay = Penalty is 5 (False Negative)\n",
    "2. If the true value was repay and we predict default = Penalty is 1 (False Positive)\n",
    "\n",
    "Then the optimal decision threshold is\n",
    "$$\\frac{\\textrm{False Positive}}{\\textrm{False Positive} + \\textrm{False Negative}} = \\frac{1}{1 + 5} = 0.167$$\n",
    "\n",
    "First recall that our model does not predict 0 or 1. Rather it predictions a probability value, i.e. a number between 0 and 1. The modeller then needs to set a threshold at which we classify things as 0 or 1.\n",
    "\n",
    "In particular, here our model is predicting if someone defaults. So if your model prediction was $\\hat{Y}=0.2$ then\n",
    "$$\\mathbb{P}[\\textrm{Customer Default}] = 0.2$$\n",
    "Again, we are not classifying anything yet, we are only giving a probability. Now depending on the threshold you set the classification will be different.\n",
    "\n",
    "So if the threshold was $\\tau=0.167$ then if the model prediction was $\\hat{Y}=0.2\\implies \\textrm{Default}$.\n",
    "\n",
    "However, if the threshold was $\\tau=0.25$, then if the model prediction was $\\hat{Y}=0.2\\implies \\textrm{Not Default}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the threshold sets the level of confidence you require your model to have. A low threshold would mean that the model requires very little confidence to predict default. While a high threshold would mean that the model requires high confidence to predict default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consider Court\n",
    "\n",
    "    It is better that ten guilty persons escape than that one innocent suffer\n",
    "    - Blackston's ratio\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/ Predicted</th>\n",
    "    <th>Innocent</th>\n",
    "     <th>Guilty</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Innocent</th>\n",
    "    <td>0</td>\n",
    "    <td>10</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Guilty</th>\n",
    "    <td>1</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "The decision threshold is \n",
    "$$\\frac{10}{11}\\approx 91\\%$$\n",
    "\n",
    "That is, if $\\hat{Y} < 91\\% \\to \\textrm{Innocent}$ and $\\hat{Y} \\geq 91\\% \\to \\textrm{Guilty}$. This is what we mean when we say \"guilty beyond a reasonable doubt\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Validation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th>Actual/ Predicted</th>\n",
    "    <th>Repayment</th>\n",
    "     <th>Default</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Repayment</th>\n",
    "    <td>0</td>\n",
    "    <td>1</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Default</th>\n",
    "    <td>5</td>\n",
    "    <td>0</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "Equivalently\n",
    "\n",
    "\n",
    "|            | $Y_{pred} = 0$ | $Y_{pred} = 1$ |\n",
    "|------------|--------------|--------------|\n",
    "|$Y_{true} = 0$|        0     |     1      | \n",
    "|$Y_{true} = 1$|  5         |      0       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
